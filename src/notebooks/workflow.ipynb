{"cells":[{"cell_type":"markdown","metadata":{},"source":["# End-to-End Food Taste Profiling and Evaluation\n","\n","This notebook implements the complete workflow for generating and evaluating taste profiles for a list of unlabeled foods. It uses the knowledge base generated in the previous steps, combined with an LLM. Evaluation is done via an LLM as a Judge."]},{"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["%pip install transformers torch accelerate bitsandbytes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:52:21.933426Z","iopub.status.busy":"2025-09-04T17:52:21.933081Z","iopub.status.idle":"2025-09-04T17:52:21.939688Z","shell.execute_reply":"2025-09-04T17:52:21.938657Z","shell.execute_reply.started":"2025-09-04T17:52:21.933399Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","import pandas as pd\n","import io\n","import csv\n","import time\n","from huggingface_hub import login\n","\n","from getpass import getpass\n","import ast\n","import json\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:45:50.102912Z","iopub.status.busy":"2025-09-04T17:45:50.102662Z","iopub.status.idle":"2025-09-04T17:45:53.518755Z","shell.execute_reply":"2025-09-04T17:45:53.517821Z","shell.execute_reply.started":"2025-09-04T17:45:50.102893Z"},"trusted":true},"outputs":[],"source":["HUGGINGFACE_API_KEY = getpass(\"Enter your Hugging Face API key: \")\n","login(token=HUGGINGFACE_API_KEY)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:45:55.295857Z","iopub.status.busy":"2025-09-04T17:45:55.295474Z","iopub.status.idle":"2025-09-04T17:45:55.303371Z","shell.execute_reply":"2025-09-04T17:45:55.302528Z","shell.execute_reply.started":"2025-09-04T17:45:55.295830Z"},"trusted":true},"outputs":[],"source":["MODEL = \"mistralai/Mistral-Nemo-Instruct-2407\"\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:45:57.256164Z","iopub.status.busy":"2025-09-04T17:45:57.255453Z","iopub.status.idle":"2025-09-04T17:51:04.561182Z","shell.execute_reply":"2025-09-04T17:51:04.560374Z","shell.execute_reply.started":"2025-09-04T17:45:57.256133Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    MODEL,\n","    quantization_config=quantization_config,\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","print(\"Model and tokenizer loaded successfully.\")"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Taste Profile Generation\n","\n","This section contains all the logic for converting an unlabeled food item into a labeled one with a 5-point taste profile."]},{"cell_type":"markdown","metadata":{},"source":["### Model Prompts - Ingredient Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["SYSTEM_PROMPT = \"\"\"You are a culinary expert and food scientist. Your task is to analyze a food item's name and description to identify its key ingredients and their contribution to the overall taste profile.\n","\n","CONTEXT:\n","- You will be given the name and description of a food item.\n","- Some ingredients will be explicitly mentioned, while others are implicit and require your internal knowledge (e.g., \"pad thai\" implies rice noodles, peanuts, egg, a protein, salt, etc.).\n","- You must determine the \"taste proportion\" of each ingredient, which represents its impact on the final taste, not its volume or weight. For instance, a small amount of a potent ingredient like wasabi will have a high taste proportion.\n","- The sum of all taste proportions for a given food item should ideally add up to 1.0.\n","\n","RULES:\n","1.  **Identify Ingredients:** Extract all key ingredients, both explicit and implicit.\n","2.  **Use Processed Names:** If an ingredient is processed in a way that significantly alters its taste, use the format \"ingredient_processed\" (e.g., \"cucumber_pickled\", \"pork_cured\", \"cabbage_fermented\"). If it's a basic ingredient, use its normal name (e.g. \"bread\", \"tofu\").\n","3.  **Estimate Taste Proportion:** Assign a decimal value from 0.0 to 1.0 for each ingredient's taste proportion.\n","4.  **Format Output Correctly:** The output MUST be only a list of CSV lines. Do not include headers, explanations, or any other text.\n","\n","**OUTPUT FORMAT** STRICTLY ONLY RETURN a valid JSON list of dictionaries, enclosed in square brackets [ ... :\n","[{ingredient: ingredient_name,taste_proportion: taste_proportion}, {}, ...]\n","\n","EXAMPLES:\n","Input:\n","Name: Sriracha\n","Description: A type of hot sauce or chili sauce made from a paste of chili peppers, distilled vinegar, garlic, sugar, and salt.\n","\n","Output:\n","[{'ingredient': 'chili_peppers_fermented', 'taste_proportion': 0.5},\n","{'ingredient': 'vinegar_distilled', 'taste_proportion': 0.2},\n","{'ingredient': 'sugar', 'taste_proportion': 0.15},\n","{'ingredient': 'garlic', 'taste_proportion': 0.1},\n","{'ingredient': 'salt', 'taste_proportion': 0.05}]\n","\n","Input:\n","Name: Guacamole\n","Description: An avocado-based dip, spread, or salad first developed in Mexico, often containing lime juice, cilantro, and onions.\n","\n","Output:\n","[{'ingredient': avocado, 'taste_proportion': 0.5},\n","{'ingredient': lime, 'taste_proportion': 0.2},\n","{'ingredient': onion, 'taste_proportion': 0.15},\n","{'ingredient': cilantro, 'taste_proportion': 0.1},\n","{'ingredient': salt, 'taste_proportion': 0.05}]\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["USER_PROMPT_TEMPLATE = \"\"\"\n","Generate the ingredient and taste proportion list for this food item:\n","Name: {food_name}\n","Description: {food_description}\n","\n","**IMPORTANT**: Your response must contain ONLY the raw CSV lines and nothing else.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prompts for when the knowledge base does not have the requisite ingredient\n","SYSTEM_PROMPT_FALLBACK = \"\"\"You are a food science expert. Your task is to estimate the 5-point taste profile for a given ingredient.\n","\n","RULES:\n","1.  Analyze the ingredient name.\n","2.  Estimate the scores for salty, umami, sweet, sour, and bitter on a scale from 0.0 to 1.0.\n","3.  Your response MUST BE ONLY a valid JSON object with the five taste keys. Do not include any other text, explanations, or markdown.\n","\n","**OUTPUT FORMAT** STRICTLY ONLY RETURN a valid JSON object:\n","{\"salty\": 0.0, \"umami\": 0.0, \"sweet\": 0.0, \"sour\": 0.0, \"bitter\": 0.0}\n","\n","EXAMPLES:\n","Input: lime\n","Output:\n","{\"salty\": 0.0, \"umami\": 0.1, \"sweet\": 0.1, \"sour\": 0.8, \"bitter\": 0.1}\n","\n","Input: soy_sauce\n","Output:\n","{\"salty\": 0.9, \"umami\": 0.8, \"sweet\": 0.2, \"sour\": 0.1, \"bitter\": 0.2}\n","\"\"\"\n","\n","USER_PROMPT_FALLBACK_TEMPLATE = \"Ingredient: {ingredient_name}\""]},{"cell_type":"markdown","metadata":{},"source":["### Generation Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"/kaggle/input/food-dataset/data/food_items_unlabeled.csv\") # replace with the unlabeled dataset path\n","food_name = df.iloc[1]['name']\n","food_description = df.iloc[1]['description']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def extract_ingredients(food_name: str, food_description: str, model, tokenizer) -> str:\n","    # Generates the ingredient list and taste proportions for a given food item using the LLM.\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n","        {\"role\": \"user\", \"content\": USER_PROMPT_TEMPLATE.format(\n","            food_name=food_name,\n","            food_description=food_description\n","        )}\n","    ]\n","\n","    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=1024,\n","        do_sample=False\n","    )\n","\n","    output_text = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n","    return output_text.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_taste_profile_from_llm(ingredient_name: str, model, tokenizer):\n","    # Uses the LLM to generate a taste profile for a single ingredient not found in the knowledge base.\n","    \n","    print(f\"Ingredient '{ingredient_name}' not in KB. Querying LLM\")\n","    messages = [\n","        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_FALLBACK},\n","        {\"role\": \"user\", \"content\": USER_PROMPT_FALLBACK_TEMPLATE.format(ingredient_name=ingredient_name)}\n","    ]\n","\n","    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n","    output_text = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()\n","\n","    try:\n","        profile = json.loads(output_text)\n","        if all(key in profile for key in ['salty', 'umami', 'sweet', 'sour', 'bitter']):\n","            print(f\"  -> LLM generated profile for '{ingredient_name}'.\")\n","            return profile\n","        else:\n","            raise ValueError(\"Missing keys in LLM JSON output.\")\n","    except (json.JSONDecodeError, ValueError) as e:\n","        print(f\"Failed to parse LLM fallback response for '{ingredient_name}': {e}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_weighted_taste_profile(llm_output_str: str, knowledge_base_path: str, model, tokenizer):\n","    # parses ingredient output, looks them up in the knowledge base, and uses an LLM fallback for missing ingredients to calculate a weighetd average taste profile\n","\n","    try:\n","        kb_df = pd.read_csv(knowledge_base_path).set_index('entity_name')\n","    except FileNotFoundError:\n","        print(f\"Error: Knowledge base file not found at '{knowledge_base_path}'\")\n","        return None\n","\n","    try:\n","        ingredients = json.loads(llm_output_str)\n","        if not isinstance(ingredients, list): raise ValueError(\"LLM output is not a list.\")\n","    except (ValueError, json.JSONDecodeError) as e:\n","        print(f\"Error parsing LLM output: {e}\")\n","        return None\n","\n","    final_profile = {'salty': 0.0, 'umami': 0.0, 'sweet': 0.0, 'sour': 0.0, 'bitter': 0.0}\n","    taste_cols = list(final_profile.keys())\n","\n","    for item in ingredients:\n","        name = item.get('ingredient')\n","        proportion = float(item.get('taste_proportion', 0.0))\n","        ingredient_profile = None\n","\n","        if name in kb_df.index:\n","            # 1. get ingredient from KB\n","            ingredient_profile = kb_df.loc[name, taste_cols].to_dict()\n","        else:\n","            # 2. fallback to LLM if it does not exist\n","            ingredient_profile = get_taste_profile_from_llm(name, model, tokenizer)\n","\n","        # 3. add to list\n","        if ingredient_profile:\n","            for taste in taste_cols:\n","                final_profile[taste] += float(ingredient_profile[taste]) * proportion\n","\n","    return final_profile"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_food_data(input_csv: str, output_csv: str, knowledge_base_path: str, model, tokenizer):\n","    # reads unlabeled food data, processes each item to get a taste profile, and saves the labeled data\n","\n","    print(f\"Starting food data processing for '{input_csv}'\")\n","    df = pd.read_csv(input_csv)\n","    results = []\n","\n","    for index, row in df.iterrows():\n","        start_time = time.time()\n","        food_name = row['name']\n","        \n","        print(f\"[{index + 1}/{len(df)}] Processing: '{food_name}'...\")\n","\n","        # 1. Ingredient proportions from LLM\n","        ingredient_json_str = extract_ingredients(food_name, row['description'], model, tokenizer)\n","        print(f\"  -> Got ingredients from LLM.\")\n","\n","        # 2. Taste profile from KB\n","        taste_profile = calculate_weighted_taste_profile(ingredient_json_str, knowledge_base_path, model, tokenizer)\n","\n","        if taste_profile:\n","            # 3. Best Label\n","            best_label = max(taste_profile, key=taste_profile.get)\n","            taste_profile['best_label'] = best_label\n","            print(f\"Best Label: '{best_label}'.\")\n","        else:\n","            print(f\"Failed to calculate profile for '{food_name}'. Skipping.\")\n","            # empty profile for failed rows\n","            taste_profile = {\n","                'salty': 0.0, 'umami': 0.0, 'sweet': 0.0, 'sour': 0.0, 'bitter': 0.0, 'best_label': 'unknown'\n","            }\n","\n","        new_row = {**row.to_dict(), **taste_profile}\n","        results.append(new_row)\n","\n","        end_time = time.time()\n","        print(f\"Done in {end_time - start_time:.2f} seconds.\\n\")\n","\n","\n","    # save to new csv\n","    output_df = pd.DataFrame(results)\n","    cols = ['id', 'name', 'description', 'best_label', 'salty', 'umami', 'sweet', 'sour', 'bitter']\n","    output_df = output_df[cols]\n","\n","    output_df.to_csv(output_csv, index=False)\n","    print(f\"Processing complete. Labeled data saved to '{output_csv}'.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Workflow Execution"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# replace these with your actual file paths\n","INPUT_CSV_PATH = \"/kaggle/input/food-dataset/data/food_items_unlabeled.csv\"\n","KNOWLEDGE_BASE_PATH = \"/kaggle/input/food-dataset/data/knowledge_base_average_processed.csv\"\n","OUTPUT_CSV_PATH = \"food_items_labeled.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["process_food_data(INPUT_CSV_PATH, OUTPUT_CSV_PATH, KNOWLEDGE_BASE_PATH, model, tokenizer)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Workflow"]},{"cell_type":"markdown","metadata":{},"source":["### Model Prompts - Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:51:12.485526Z","iopub.status.busy":"2025-09-04T17:51:12.484838Z","iopub.status.idle":"2025-09-04T17:51:12.490456Z","shell.execute_reply":"2025-09-04T17:51:12.489356Z","shell.execute_reply.started":"2025-09-04T17:51:12.485481Z"},"trusted":true},"outputs":[],"source":["SYSTEM_PROMPT_JUDGE = \"\"\"You are an impartial food science expert acting as an automated judge. Your sole task is to evaluate the plausibility of a dominant taste label assigned to a food item by another system.\n","\n","You will be given the food's name, its description, the system's chosen dominant taste ('best_label'), and its runner-up choice ('second_best_label').\n","\n","Based on this information, you must categorize the system's choice by responding with a single letter (A, B, C, or D) based on the following definitions:\n","\n","A) **Accurate:** The provided 'best_label' is clearly the most accurate and logical dominant taste for this food item.\n","B) **Acceptable:** The 'best_label' is a reasonable choice, but the 'second_best_label' (or another unlisted taste) is equally or more dominant. The choice is defensible but not definitively the best.\n","C) **Inaccurate:** The 'best_label' is clearly incorrect. A different taste is obviously dominant.\n","D) **No Single Dominant Taste:** The food item is complex and does not have one single dominant taste that stands out significantly above others.\n","\n","**IMPORTANT RULE:** Your entire response MUST be only the single capital letter (A, B, C, or D) that best fits the evaluation. Do not provide any explanation, context, or additional text.\n","\"\"\"\n","\n","USER_PROMPT_JUDGE_TEMPLATE = \"\"\"Food Name: {name}\n","Description: {description}\n","System's best_label: {best_label}\n","System's second_best_label: {second_best_label}\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_evaluation_grade(row: pd.Series, model, tokenizer) -> str:\n","    # Evaluates a single labeled food item using the LLM as a Judge\n","    \n","    # get the second best label\n","    taste_cols = ['salty', 'umami', 'sweet', 'sour', 'bitter']\n","    scores = row[taste_cols].to_dict()\n","    best_label = row['best_label']\n","    \n","    scores.pop(best_label, None)\n","    second_best_label = max(scores, key=scores.get) if scores else \"N/A\"\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_JUDGE},\n","        {\"role\": \"user\", \"content\": USER_PROMPT_JUDGE_TEMPLATE.format(\n","            name=row['name'],\n","            description=row['description'],\n","            best_label=best_label,\n","            second_best_label=second_best_label\n","        )}\n","    ]\n","    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    outputs = model.generate(**inputs, max_new_tokens=5, do_sample=False)\n","    output_text = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()\n","\n","    # parse output to get a letter grade\n","    match = re.search(r'^[A-D]', output_text)\n","    if match:\n","        return match.group(0)\n","    else:\n","        print(f\"  -> Warning: Could not parse grade from LLM output: '{output_text}'\")\n","        return \"Error\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:55:06.578526Z","iopub.status.busy":"2025-09-04T17:55:06.577582Z","iopub.status.idle":"2025-09-04T17:55:06.586013Z","shell.execute_reply":"2025-09-04T17:55:06.585180Z","shell.execute_reply.started":"2025-09-04T17:55:06.578472Z"},"trusted":true},"outputs":[],"source":["def augment_with_evaluations(input_csv: str, output_csv: str, model, tokenizer, sample_size: int = None):\n","    # reads a labeled dataset and gets an evaluation grade for each item, saving to a new CSV\n","\n","    print(f\"Starting evaluation process for '{input_csv}'\")\n","    try:\n","        df = pd.read_csv(input_csv)\n","    except FileNotFoundError:\n","        print(f\"Error: Labeled data file not found at '{input_csv}'\")\n","        return\n","\n","    if sample_size and sample_size < len(df):\n","        print(f\"Processing a random sample of {sample_size} items.\")\n","        eval_df = df.sample(n=sample_size, random_state=42)\n","    else:\n","        print(f\"Processing all {len(df)} items.\")\n","        eval_df = df\n","\n","    grades = []\n","    total_rows = len(eval_df)\n","    for i, (index, row) in enumerate(eval_df.iterrows()):\n","        print(f\"Processing item {i + 1}/{total_rows}: '{row['name']}'...\")\n","        grade = get_evaluation_grade(row, model, tokenizer)\n","        grades.append(grade)\n","\n","    eval_df['eval_grade'] = grades\n","    eval_df.to_csv(output_csv, index=False)\n","    print(f\"\\nProcessing complete. Augmented data saved to '{output_csv}'.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:55:06.945335Z","iopub.status.busy":"2025-09-04T17:55:06.944997Z","iopub.status.idle":"2025-09-04T17:55:06.952589Z","shell.execute_reply":"2025-09-04T17:55:06.951792Z","shell.execute_reply.started":"2025-09-04T17:55:06.945309Z"},"trusted":true},"outputs":[],"source":["def report_metrics_from_csv(evaluation_csv: str):\n","    # prints final evaluation report\n","\n","    print(f\"\\n--- Reading Evaluation Report from '{evaluation_csv}' ---\")\n","    try:\n","        df = pd.read_csv(evaluation_csv)\n","    except FileNotFoundError:\n","        print(f\"Error: Evaluation file not found at '{evaluation_csv}'\")\n","        return\n","\n","    grades = df['eval_grade'].tolist()\n","    total_evaluated = len([g for g in grades if g != 'Error'])\n","    if total_evaluated == 0:\n","        print(\"No valid evaluations found in the file.\")\n","        return\n","\n","    count_a = grades.count('A')\n","    count_b = grades.count('B')\n","    count_c = grades.count('C')\n","    count_d = grades.count('D')\n","\n","    strict_accuracy = count_a / total_evaluated\n","    acceptable_accuracy = (count_a + count_b) / total_evaluated\n","\n","    print(f\"Total Items Evaluated: {total_evaluated}\")\n","    print(\"-\" * 25)\n","    print(f\"Grade 'A' (Accurate):         {count_a}\")\n","    print(f\"Grade 'B' (Acceptable):       {count_b}\")\n","    print(f\"Grade 'C' (Inaccurate):       {count_c}\")\n","    print(f\"Grade 'D' (No Dominant):      {count_d}\")\n","    print(\"-\" * 25)\n","    print(f\"Strict Accuracy (A only):       {strict_accuracy:.2%}\")\n","    print(f\"Acceptable Accuracy (A + B):    {acceptable_accuracy:.2%}\")\n","    print(\"---------------------------\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["### Run Evaluation Workflow"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:55:09.168242Z","iopub.status.busy":"2025-09-04T17:55:09.167884Z","iopub.status.idle":"2025-09-04T17:55:09.172691Z","shell.execute_reply":"2025-09-04T17:55:09.171735Z","shell.execute_reply.started":"2025-09-04T17:55:09.168217Z"},"trusted":true},"outputs":[],"source":["# replace these with your actual file paths\n","LABELED_CSV_PATH = \"food_items_labeled.csv\"\n","EVALUATION_CSV_PATH = \"food_items_evaluation.csv\""]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:55:22.317354Z","iopub.status.busy":"2025-09-04T17:55:22.316680Z","iopub.status.idle":"2025-09-04T17:59:13.365343Z","shell.execute_reply":"2025-09-04T17:59:13.364294Z","shell.execute_reply.started":"2025-09-04T17:55:22.317325Z"},"trusted":true},"outputs":[],"source":["augment_with_evaluations(\n","    input_csv=LABELED_CSV_PATH,\n","    output_csv=EVALUATION_CSV_PATH,\n","    model=model,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-09-04T17:59:13.366999Z","iopub.status.busy":"2025-09-04T17:59:13.366716Z","iopub.status.idle":"2025-09-04T17:59:13.383170Z","shell.execute_reply":"2025-09-04T17:59:13.382059Z","shell.execute_reply.started":"2025-09-04T17:59:13.366977Z"},"trusted":true},"outputs":[],"source":["report_metrics_from_csv(EVALUATION_CSV_PATH)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":8203321,"sourceId":12961772,"sourceType":"datasetVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":4}
